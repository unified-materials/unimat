<!doctype html>
<html lang="en-GB" class="no-js fixed-nav">
<head>
	<meta charset="UTF-8">

	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="format-detection" content="telephone=no">
	<meta name='robots' content='index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1' />

	<title>UniMat: Scalable Diffusion for Materials Generation</title>


	<link rel='stylesheet' id='wayve-all-css' href='style.css' type='text/css' media='all' />
	<link rel='stylesheet' id='wayve-all-css' href='custom.css' type='text/css' media='all' />
	<script src="script.js" defer></script>
	<script src="bootstrap.js"></script>
	<link rel="stylesheet" href="bootstrap-grid.css">
	    
</head>


<article class='blog-post'>

  <div class=blog-intro-container>
  <h1 class='too-long' style="text-align:center;">UniMat: Scalable Diffusion for Materials Generation</h1>

    <div id="authors">
        <center>
            <div class="author-row-new">
              <a href="https://sherryy.github.io/" style="color: darkblue; text-decoration: none;">Sherry Yang<sup>12</sup></a>,
              <a href="https://www.linkedin.com/in/ruben-cho" style="color: darkblue; text-decoration: none;">KwangHwan Cho<sup>2</sup></a>,
              <a href="https://scholar.google.com/citations?user=uRImMPoAAAAJ&hl=en" style="color: darkblue; text-decoration: none;">Amil Merchant<sup>1</sup></a>,	      
              <a href="http://people.eecs.berkeley.edu/~pabbeel/" style="color: darkblue; text-decoration: none;">Pieter Abbeel<sup>2</sup></a>,
              <a href="http://webdocs.cs.ualberta.ca/~dale/" style="color: darkblue; text-decoration: none;">Dale Schuurmans<sup>1</sup></a>,
              <a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en" style="color: darkblue; text-decoration: none;">Igor Mordatch<sup>1</sup></a>,
              <a href="https://research.google/people/EkinDogusCubuk/" style="color: darkblue; text-decoration: none;">Ekin Dogus Cubuk<sup>1</sup></a>,	      
            </div>
        </center>
        <center>
        <div class="affiliations">
            <span><sup>1</sup> Google DeepMind</span>
            <span><sup>2</sup> UC Berkeley</span>
        </div>
    </div>  
  </div>  
  
      
<div id="video-player-container">


<!-- Video Display -->
<div class="single-image">
    <span>
        <video id="videoPlayer" loop muted playsinline width="3840" height="2160">
            <source src="materials/unimat.mp4" type="video/mp4">
        </video>
    </span>
</div>

</div>

<div class=blog-intro-container>
  <p style="text-align:center"><a href="materials/paper.pdf" target="_blank" rel="noopener" data-display-type="button" class="custom-button">paper</a></p>  
  <div class=intro>
    <p><span style="font-weight: 400;">Generative models trained on internet-scale data are capable of generating novel and realistic texts, images, and videos. A natural next question is whether these models can advance science, for example by generating novel stable materials. Traditionally, models with explicit structures (e.g., graphs) have been used in modeling structural relationships in scientific data (e.g., atoms and bonds in crystals), but generating structures can be difficult to scale to large and complex systems. Another challenge in generating materials is the mismatch between standard generative modeling metrics and downstream applications. For instance, common metrics such as the reconstruction error do not correlate well with the downstream goal of discovering  stable materials. In this work, we tackle the scalability challenge by developing a <b>unified crystal representation</b> that can represent any crystal structure (UniMat), followed by training a diffusion probabilistic model on these  UniMat  representations. Our empirical results suggest that despite the lack of explicit structure modeling,  UniMat  can generate high fidelity crystal structures from larger and more complex chemical systems, outperforming previous graph-based approaches under various generative modeling metrics. To better connect the generation quality of materials to downstream applications, such as discovering novel stable materials, we propose additional metrics for evaluating generative models of materials, including per-composition formation energy and stability with respect to convex hulls through decomposition energy from Density Function Theory (DFT). Lastly, we show that conditional generation with  UniMat  can scale to previously established crystal datasets with up to millions of crystals structures, outperforming random structure search (the current leading method for structure discovery) in discovering new stable materials.</p>
  </div>
</div>


<div class="banner-container">
  <video class=banner loop muted playsinline autoplay height=580><source src="materials/banner.mp4"></video>
</div>

<div class="copy">
  
<!--   <div class="copy" >   -->
<!--     <div class=content id="action-rich"> -->
<!--       <h2><span style="font-weight: 400;">Action-Rich Simulations</span></h2> -->
<!--       <p><span style="font-weight: 400;">One major difference between an interactive real-world simulator and typical video generation is that a simulator should support a diverse set of actions. Below, we demonstrate how UniSim can simulate different experiences given different actions starting from the same initial frame.</span></p> -->
<!--     </div> -->
<!--   </div> -->

<!-- <div class="single-image "> -->
<!--   <span> -->
<!--     <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/kitchen_word.mp4"></video> -->
<!--   </span> -->
<!-- </div> -->

<!-- <div class="single-image "> -->
<!--   <span> -->
<!--     <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/switch_word.mp4"></video> -->
<!--   </span> -->
<!-- </div> -->

<!-- <div class="single-image "> -->
<!--   <span> -->
<!--     <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/chapel_word.mp4"></video> -->
<!--   </span> -->
<!-- </div> -->

<!-- <div class="single-image "> -->
<!--   <span> -->
<!--     <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/golden_gate_word.mp4"></video> -->
<!--   </span> -->
<!-- </div> -->


<div class="copy" >
  <div class=content id="long-horizon">    
    <h2><span style="font-weight: 400;">Long-Horizon Simulations</span></h2>
    <p>
      The true value of UniSim lies in simulating long episodes to enable optimizing decisions through search, planning, optimal control, or reinforcement learning. Below, we demonstrate how UniSim can simulate interactive experiences with long interaction horizons.
    </p>
  </div>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/fractal_word.mp4"></video>
  </span>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/play_word.mp4"></video>
  </span>
</div>


<!-- <div class="copy" > -->
<!--   <div class=content id="diversity"> -->
<!--     <h2><span style="font-weight: 400;">Diversity and Stochasticity</span></h2> -->
<!--     <p> -->
<!--       UniSim can also support highly diverse and stochastic environment transitions, such as diversity in objects being revealed after removing the towel, as well as locations and colors of objects being placed, as illustrated below. -->
<!--     </p> -->
<!--   </div> -->
<!-- </div> -->

<!-- <div class="single-image "> -->
<!--   <span> -->
<!--     <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/uncover_word.mp4"></video> -->
<!--   </span> -->
<!-- </div> -->

<!-- <div class="single-image "> -->
<!--   <span> -->
<!--     <video loop muted playsinline autoplay width=3840 height=1060><source src="materials/put_word.mp4"></video> -->
<!--   </span> -->
<!-- </div> -->

<div class="copy" >
  <div class=content id="rl">    
    <h2><span style="font-weight: 400;">Reinforcement Learning with UniSim</span></h2>
    <p> 
      UniSim allows effective training of RL agents <b>purely in simulation</b>, which can be directly transferred onto real robot. This can pave the way to training policies without expensive real world intervention. The simulated rollout of the RL policy is shown below:
    </p>
  </div>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/rl_sim_video-slow.mp4"></video>
  </span>
</div>

<div class="copy" >
  <div class=content>
    <p> We then deploy the RL policy trained in UniSim onto the real robot in <b>zero-shot</b>. The real-robot executions are shown below: </p>
  </div>
</div>


<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/rl_real_video-fast.mp4"></video>
  </span>
</div>

<div class="copy" >
  <div class=content id="planning">    
    <h2><span style="font-weight: 400;">Long-Horizon Planning with UniSim</span></h2>
    <p>
      UniSim can be used to train embodied planners <b> purely in simulation</b>. We concatenate long-horizon instructions and generate videos by repeated rollouts in UniSim. The resulting videos and instructions can then be used to train goal conditioned vision-language model (VLM) policies. The simulated plans and <b> zero-shot </b> transfer to real robot are shown below.
    </p>
  </div>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/hindsight_word.mp4"></video>
  </span>
</div>


<div class="copy" >
  <div class=content id="related">    
    <h2><span style="font-weight: 400;">Related Resources</span></h2>

        <div class="row vspace-top">
        <div class="col-sm-3">
            <div class="move-down">
                <img src="materials/fmdm.png" class="img-fluid" alt="FMDM" style="width:100%">
            </div>
        </div>
        <div class="col-sm-9">
          <div class="paper-title">
            <a href="https://sites.google.com/corp/view/fmdm-neurips23/" style="color: darkblue; text-decoration: none;">Foundation Models for Decision Making NeurIPS 2023 Workshop</a>
        </div>
          <div>
	    The FMDM workshop brings together the decision making community and the foundation models community in vision and language to confront the challenges in decision making at scale.
        </div>
        </div>
        </div>

	<br>
        <div class="row vspace-top">
        <div class="col-sm-3">
            <div class="move-down">
                <img src="materials/unipi.png" class="img-fluid" alt="UniPi" style="width:100%">
            </div>
        </div>
        <div class="col-sm-9">
          <div class="paper-title">
            <a href="https://universal-policy.github.io/" style="color: darkblue; text-decoration: none;">Learning Universal Policies via Text-Guided Video Generation</a>
        </div>
          <div>
	    UniPi casts sequential decision making as a text-conditioned video generation problem. UniPi produces policies that can generalize to combinatorial and multi-task environments and able to utilize broad internet-scale text-video datasets.
        </div>
        </div>
        </div>

	<br>
        <div class="row vspace-top">
        <div class="col-sm-3">
            <div class="move-down">
                <img src="materials/video_adapter.png" class="img-fluid" alt="VideoAdapter" style="width:100%">
            </div>
        </div>
        <div class="col-sm-9">
          <div class="paper-title">
            <a href="https://video-adapter.github.io//" style="color: darkblue; text-decoration: none;">Probabilistic Adaptation of Black-Box Text-to-Video Models</a>
        </div>
          <div> Video Adapter is a framework for generating personalized, stylized, and domain-specific videos by leveraging pretrained black-box text-to-video diffusion models as a probabilistic prior without requiring access to pretrained model weights.
        </div>
        </div>
        </div>	

  </div>
</div>

    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@article{yang2023learning,
	    title={Learning Interactive Real-World Simulators},
	    author={Yang, Mengjiao and Du, Yilun and Ghasemipour, Kamyar and Tompson, Jonathan and Schuurmans, Dale and Abbeel, Pieter},
	    journal={arXiv e-prints},
	    pages={arXiv--2310},
	    year={2023}
	    }</code></pre>
    </section>


</html>


